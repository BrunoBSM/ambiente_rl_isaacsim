# Ambiente RL Isaac Sim - Go2 Multi-Robot

Sistema de treinamento de Reinforcement Learning para rob√¥s Go2 usando Isaac Sim e CleanRL.

## Estrutura do Ambiente

```
ambiente_rl_isaacsim/
‚îú‚îÄ‚îÄ cleanrl_isaacsim/           # Algoritmos RL customizados
‚îÇ   ‚îú‚îÄ‚îÄ algorithms/             # Implementa√ß√µes de algoritmos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ppo_isaacsim.py    # PPO para Isaac Sim
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py           # Utilit√°rios
‚îÇ   ‚îú‚îÄ‚îÄ envs/                  # Ambientes customizados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ go2_env.py         # Ambiente Go2
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi_env_wrapper.py # Wrapper multi-ambiente
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wrappers.py        # Wrappers adicionais
‚îÇ   ‚îî‚îÄ‚îÄ utils/                 # Utilit√°rios gerais
‚îÇ       ‚îú‚îÄ‚îÄ evaluation.py     # Avalia√ß√£o de modelos
‚îÇ       ‚îî‚îÄ‚îÄ wandb_utils.py     # Integra√ß√£o WandB
‚îú‚îÄ‚îÄ core/                      # Componentes principais
‚îÇ   ‚îú‚îÄ‚îÄ isaac_gym_multi_env.py # Ambiente multi-rob√¥
‚îÇ   ‚îú‚îÄ‚îÄ multi_sim_helper.py    # Helper para simula√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ sim_launcher.py        # Launcher da simula√ß√£o
‚îú‚îÄ‚îÄ configs/                   # Configura√ß√µes de treinamento
‚îÇ   ‚îî‚îÄ‚îÄ default_ppo.py        # Configura√ß√µes PPO
‚îú‚îÄ‚îÄ scripts/                   # Scripts de execu√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ train_ppo.py          # Script principal de treinamento
‚îÇ   ‚îú‚îÄ‚îÄ eval_model.py         # Avalia√ß√£o de modelos
‚îÇ   ‚îî‚îÄ‚îÄ sweep_hyperparams.py  # Sweep de hiperpar√¢metros
‚îú‚îÄ‚îÄ experiments/               # Resultados de experimentos
‚îú‚îÄ‚îÄ models/                    # Modelos treinados
‚îú‚îÄ‚îÄ docker/                    # Configura√ß√£o Docker
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile            # Imagem Isaac Sim + CleanRL
‚îú‚îÄ‚îÄ docker-compose.yml         # Orquestra√ß√£o Docker
‚îú‚îÄ‚îÄ run_docker.sh             # Script para iniciar container
‚îú‚îÄ‚îÄ python-cleanrl.sh         # Script para rodar script python
‚îî‚îÄ‚îÄ stop.sh                   # Script para parar container
```

## Instala√ß√£o e Utiliza√ß√£o com Docker

### Pr√©-requisitos

- Docker com suporte a GPU (nvidia-docker)
- Docker Compose
- Sistema X11 para GUI (Linux)

### Configura√ß√£o WandB (Opcional)

Para usar tracking WandB sem precisar passar as credenciais toda vez, configure o arquivo `config.env`:

1. **Criar arquivo de configura√ß√£o na raiz do reposit√≥rio**:
```bash
touch config.env
```

2. **Editar arquivo `config.env`** com suas credenciais WandB:
```bash
# Configura√ß√µes WandB para Docker Compose
# Copie este arquivo para config.env e configure suas vari√°veis

# API Key do WandB (obtenha em https://wandb.ai/settings)
WANDB_API_KEY=your_wandb_api_key_here

# Entidade/time WandB (seu usu√°rio ou organiza√ß√£o)
WANDB_ENTITY=your_username_or_team

# Projeto padr√£o WandB (pode ser sobrescrito via --wandb-project-name)
WANDB_PROJECT=your-project-name

```

> **üìù Nota**: Obtenha sua API key em [wandb.ai/settings](https://wandb.ai/settings)  
> **üîí Seguran√ßa**: O arquivo `config.env` deve estar no `.gitignore` para n√£o expor credenciais

### In√≠cio R√°pido

1. **Iniciar ambiente (primeira vez - build autom√°tico)**:
```bash
./run_docker.sh --build
```

2. **Iniciar ambiente (execu√ß√µes subsequentes)**:
```bash
./run_docker.sh
```

3. **Parar ambiente**:
```bash
./stop.sh
```

O script `run_docker.sh` automaticamente:
- Builda a imagem Isaac Sim + CleanRL (se `--build` especificado)
- Inicia o container em background
- Entra automaticamente no container
- Carrega vari√°veis WandB do arquivo `config.env` (se existir)

### Estrutura do Container

- **Workspace**: `/isaac-sim/ambiente_rl_isaacsim`
- **Python**: Ambiente virtual em `/isaac-sim/venv-cleanrl`
- **Imagem**: `ambiente-rl-isaacsim:4.5.0`

## Treinamento com Exemplos

### Flags Principais

#### Controle de Experimento
- `--exp-name`: Nome do experimento
- `--seed`: Seed para reprodutibilidade
- `--total-timesteps`: Total de passos de treinamento

#### Ambiente
- `--num-envs`: N√∫mero de rob√¥s em paralelo
- `--spacing`: Espa√ßamento entre rob√¥s (metros)
- `--use-relative-control`: Usar controle relativo
- `--relative-scale`: Escala para controle relativo
- `--safety-margin`: Margem de seguran√ßa para limites

#### Algoritmo PPO
- `--learning-rate`: Taxa de aprendizado
- `--num-steps`: Passos por rollout
- `--gamma`: Fator de desconto
- `--gae-lambda`: Lambda para GAE
- `--clip-coef`: Coeficiente de clipping
- `--ent-coef`: Coeficiente de entropia
- `--vf-coef`: Coeficiente da fun√ß√£o valor
- `--num-minibatches`: N√∫mero de mini-batches
- `--update-epochs`: √âpocas de atualiza√ß√£o

#### WandB Tracking
- `--track`: Habilitar tracking WandB
- `--wandb-project-name`: Nome do projeto WandB (padr√£o: `WANDB_PROJECT` env var)
- `--wandb-entity`: Entidade/time WandB (padr√£o: `WANDB_ENTITY` env var)

#### Visualiza√ß√£o
- `--webrtc`: Habilitar WebRTC para visualiza√ß√£o remota

### Exemplos de Uso

#### 1. Teste R√°pido (desenvolvimento)
```bash
./python-cleanrl.sh scripts/train_ppo.py \
    --num-envs 4 \
    --total-timesteps 100000 \
    --num-steps 8 \
    --exp-name "quick_test"
```

#### 2. Treinamento B√°sico com WandB
```bash
# Com vari√°veis configuradas no config.env (recomendado)
./python-cleanrl.sh scripts/train_ppo.py \
    --num-envs 16 \
    --total-timesteps 10000000 \
    --track \
    --exp-name "baseline_training"

# Ou especificando manualmente (sobrescreve config.env)
./python-cleanrl.sh scripts/train_ppo.py \
    --num-envs 16 \
    --total-timesteps 10000000 \
    --track \
    --exp-name "baseline_training"
```

#### 3. Treinamento Completo Otimizado
```bash
./python-cleanrl.sh scripts/train_ppo.py \
    --num-envs 32 \
    --total-timesteps 50000000 \
    --learning-rate 0.0026 \
    --num-steps 32 \
    --spacing 2.0 \
    --track \
    --exp-name "production_v1" \
    --anneal-lr
```

#### 4. Experimento com Controle Relativo
```bash
./python-cleanrl.sh scripts/train_ppo.py \
    --num-envs 16 \
    --use-relative-control \
    --relative-scale 0.05 \
    --learning-rate 0.002 \
    --track \
    --exp-name "relative_control_test"
```

#### 5. Treinamento Multi-Rob√¥ Massivo
```bash
./python-cleanrl.sh scripts/train_ppo.py \
    --num-envs 64 \
    --total-timesteps 100000000 \
    --spacing 1.5 \
    --num-steps 64 \
    --learning-rate 0.001 \
    --track \
    --exp-name "massive_parallel"
```

#### 6. Sweep de Hiperpar√¢metros
```bash
./python-cleanrl.sh scripts/sweep_hyperparams.py \
    --project "go2-hyperparams" \
    --entity "seu-time" \
    --count 20 \
    --base-num-envs 8 \
    --base-total-timesteps 1000000
```

#### 7. Treinamento com WebRTC (Visualiza√ß√£o Remota)
```bash
./python-cleanrl.sh scripts/train_ppo.py \
    --num-envs 16 \
    --webrtc \
    --track \
    --exp-name "remote_visualization"
```

### Configura√ß√µes WandB Avan√ßadas

#### Tracking Detalhado
```bash
./python-cleanrl.sh scripts/train_ppo.py \
    --track \
    --capture-video \
    --exp-name "detailed_analysis" \
    --num-envs 16
```

#### M√∫ltiplos Experimentos com Seeds
```bash
# Seed 1
./python-cleanrl.sh scripts/train_ppo.py --track --seed 1 --exp-name "multi_seed_1"

# Seed 42  
./python-cleanrl.sh scripts/train_ppo.py --track --seed 42 --exp-name "multi_seed_42"

# Seed 123
./python-cleanrl.sh scripts/train_ppo.py --track --seed 123 --exp-name "multi_seed_123"
```

### Monitoramento

- **WandB Dashboard**: `https://wandb.ai/[entity]/[project]`
- **TensorBoard**: `tensorboard --logdir experiments/runs`
- **Logs do Container**: `docker compose logs -f isaac-sim`

> **‚ö†Ô∏è Nota sobre WandB**: 
> - **Recomendado**: Configure suas credenciais no arquivo `config.env` (veja se√ß√£o "Configura√ß√£o WandB")
> - **Alternativa**: Na primeira execu√ß√£o com `--track`, o WandB solicitar√° sua API key. Obtenha em [wandb.ai/settings](https://wandb.ai/settings)
> - Se n√£o configurar `WANDB_ENTITY`, use `--wandb-entity seu_usuario` no comando

### Avalia√ß√£o de Modelos

```bash
./python-cleanrl.sh scripts/eval_model.py \
    --model-path "models/ppo_model.pt" \
    --num-envs 4 \
    --num-episodes 10
``` 